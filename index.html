<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/ri_logo.png">
  <title>Shikhar Bahl</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Shikhar Bahl</name>
              </p>
              <p>Hi there! I am a first year PhD student at the <a href="https://www.ri.cmu.edu/">Robotics Institute</a>, within the <a href="https://www.cs.cmu.edu/"> School of Computer Science</a> at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>. I am interested in artificial intelligence, machine learning and robotics.
                I am currently advised by <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a>. </p>

                <p> Prior to CMU, I did my undergrad at <a href="https://www.berkeley.edu/">UC Berkeley</a> in Applied Math and Computer Science, where I was affiliated with <a href="https://bair.berkeley.edu/">Berkeley Artificial Intelligence Research</a> (BAIR) and worked under <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> on problems in deep reinforcement learning and robotics.
              </p>


              <p> Feel free to contact me via email! You can reach me at sbahl2 -at- andrew dot cmu dot edu</p>


              <p align=center>
                <!-- <a href="mailto:jonbarron@gmail.com">Email</a> &nbsp/&nbsp -->
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp -->
                <a href="mailto:sbahl2@andrew.cmu.edu">email</a> &nbsp/&nbsp
                <a href="data/shikharCV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=bdHgGgEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/shikharbahl"> Twitter </a> &nbsp/&nbsp
                <a href="https://github.com/shikharbahl">GitHub</a>
              </p>
            </td>
            <td width="33%">
              <img src="images/rsz_shikhar_bahl_circle.png">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                I am broadly interested in creating robust autonomous agents that operate with minimal or no human supervision. More specifically, my research focuses on combining machine learning, reinforcement learning, computer vision and perception for robotic control. I am interested in making real world robotic learning more through efficient optimization. Here is some of my work:
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tr>
            <td width="25%"><img src="images/visual_natural_reward.png" width="200"  height="120" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1906.05841.pdf">
                    <papertitle>Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Rewards</papertitle>
                  </a>
                  <br>
                  Gerrit Schoettler*, <a href="http://ashvin.me/">Ashvin Nair*</a>, Jianlan Luo, <strong>Shikhar Bahl</strong>, Juan aparicio Ojea, Eugen Solowjow, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                  <br>
                  <em>arXiv preprint</em>
                  <br>
                  <a href="https://arxiv.org/pdf/1906.05841.pdf">pdf</a> | <a href="https://industrial-insertion-rl.github.io/">project page</a>
                  <!-- <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /
                  <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a> -->
                </p>
                <p>
                  We consider a variety of difficult industrial insertion tasks with visual inputs and different natural reward specifications, namely sparse rewards and goal images. We show that methods that combine RL with prior information, such as classical controllers or demonstrations, can solve these tasks from a reasonable amount of real-world interaction.
                </p>
              </td>
          </tr>


          <tr>
            <td width="25%"><img src="images/skewfit_cropped.png" width="160" height="100" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://arxiv.org/pdf/1903.03698.pdf">
                    <papertitle>Skew-Fit: State-Covering Self-Supervised Reinforcement Learning</papertitle>
                  </a>
                  <br>
                  <a href="http://people.eecs.berkeley.edu/~vitchyr/">Vitchyr H. Pong*</a>, Murtaza Dalal*, Steven Lin*, <a href="http://ashvin.me/">Ashvin Nair</a>, <strong>Shikhar Bahl</strong>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                  <br>
                  <em> Task-Agnostic Reinforcement Learning Workshop at International Conference on Learning Representations (ICLR),</em> 2019  <font color="red"><strong>(Contributed Talk)</strong></font>
                  <br>
                  <a href="https://arxiv.org/pdf/1903.03698.pdf">pdf</a> | <a href="https://sites.google.com/view/skew-fit">project page</a>
                  <!-- <a href="data/ArbelaezCVPR2014.bib">bibtex</a> -->
                </p>
                <p> We present an algorithm called Skew-Fit for learning such a maximum-entropy goal distribution, and show that under certain regularity conditions, our method converges to a uniform distribution over the set of valid states, even when we do not know this set beforehand.
                </p>
                <br>
                </p>
              </td>
          </tr>



          <tr>
            <td width="25%"><img src="images/residual.png" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794127">
                    <papertitle>Residual Reinforcement Learning for Robot Control</papertitle>
                  </a>
                  <br>
                  Tobias Johannink*, <strong>Shikhar Bahl*</strong>, <a href="http://ashvin.me/">Ashvin Nair*</a>, Jianlan Luo, Avinash Kumar, Matthias Loskyll, Juan Aparicio Ojea,  Eugen Solowjow, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                  <br>
                  <em> IEEE Conference on Robotics and Automation (ICRA), </em> 2019

                  <br>
                  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794127">pdf</a> | <a href="https://residualrl.github.io/">project page</a>
                  <!-- <br>
                  <!-- <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
                  <a href="data/ArbelaezCVPR2014.bib">bibtex</a> -->
                </p>
                <p> We study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects
                </p>
                <br>
                </p>
              </td>
          </tr>


          <tr>
              <td width="25%"><img src="images/rig.png" width="160"  style="border-style: none">
                <td width="75%" valign="top">
                  <p>
                    <a href="http://papers.nips.cc/paper/8132-visual-reinforcement-learning-with-imagined-goals.pdf">
                      <papertitle>Visual Reinforcement Learning with Imagined Goals</papertitle>
                    </a>
                    <br>
                    <a href="http://ashvin.me/">Ashvin Nair*</a>, <a href="http://people.eecs.berkeley.edu/~vitchyr/">Vitchyr H. Pong*</a>, Murtaza Dalal, <strong>Shikhar Bahl</strong>, Steven Lin, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                    <br>
                    <em> Advances in Neural Information Processing Systems (NIPS), </em> 2018  <font color="red"><strong>(Spotlight Presentation)</strong></font>
                    <br>
                    <a href="http://papers.nips.cc/paper/8132-visual-reinforcement-learning-with-imagined-goals.pdf">pdf</a> | <a href="https://sites.google.com/site/visualrlwithimaginedgoals/">project page</a>
                    <!-- <a href="data/ArbelaezCVPR2014.bib">bibtex</a> -->
                  </p>
                  <p> We propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies.
                  </p>
                  <br>
                  </p>
                </td>
            </tr>








        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/SparseGraphSenators.png" width="160" height="160"></td>
            <td width="75%" valign="center">
              <p>
                <a href="https://people.eecs.berkeley.edu/~elghaoui/Teaching/EECS127/">
                  <papertitle>EECS127 - Fall 2018 (uGSI)</papertitle>
                </a>
              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                    Credit to this great <a href="https://github.com/jonbarron/jonbarron_website"><strong>repo</strong></a> for providing the source code for the website!
                  </font>
              </p>
            </td>
          </tr>
        </table>
        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
